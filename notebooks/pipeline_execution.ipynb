{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767382dd",
   "metadata": {},
   "source": [
    "# Credit Risk Modeling Pipeline Execution\n",
    "\n",
    "This notebook executes the complete credit risk modeling pipeline based on the main.py script. It demonstrates the entire end-to-end process from data loading to model stacking and prediction generation for credit risk assessment.\n",
    "\n",
    "## Project Objective\n",
    "The goal is to predict the probability of loan default for credit applicants using advanced machine learning techniques. This is a binary classification problem where we predict whether a customer will have payment difficulties (TARGET = 1) or not (TARGET = 0).\n",
    "\n",
    "## Dataset Information\n",
    "- **Source**: Home Credit Default Risk competition dataset\n",
    "- **Training Data**: Historical loan applications with known outcomes\n",
    "- **Test Data**: New applications requiring default probability predictions\n",
    "- **Features**: Demographic, financial, and historical credit information\n",
    "\n",
    "## Pipeline Overview:\n",
    "\n",
    "### 1. **Data Loading & Preprocessing** \n",
    "- Load raw datasets (application_train.csv, application_test.csv)\n",
    "- Merge with additional data sources (bureau, previous applications, etc.)\n",
    "- Basic data quality checks and initial preprocessing\n",
    "\n",
    "### 2. **Feature Engineering** \n",
    "- Automatic creation of new predictive features\n",
    "- Aggregation of external data sources\n",
    "- Ratio calculations and interaction features\n",
    "- Time-based feature engineering\n",
    "\n",
    "### 3. **Data Quality & Encoding**\n",
    "- Handle missing values using domain knowledge\n",
    "- Target encoding for categorical variables\n",
    "- Data validation and quality checks\n",
    "\n",
    "### 4. **Feature Selection**\n",
    "- SHAP-based feature importance (default)\n",
    "- Variance-based selection (fast alternative)\n",
    "- Correlation filtering to remove redundant features\n",
    "- Select top 50 most predictive features\n",
    "\n",
    "### 5. **Three-Level Stacking Approach**\n",
    "- **L1 (Base Models)**: XGBoost, LightGBM, CatBoost - diverse algorithms for robust predictions\n",
    "- **L2 (Meta Models)**: ExtraTrees, Logistic Regression - combine L1 predictions intelligently\n",
    "- **L3 (Final Ensemble)**: ExtraTrees on L2 outputs + raw features for final prediction\n",
    "\n",
    "### 6. **Model Evaluation & Output**\n",
    "- Cross-validation performance metrics (AUC-ROC)\n",
    "- Out-of-fold predictions for model validation\n",
    "- Final submission file generation\n",
    "\n",
    "## Key Features:\n",
    "- **Modular Design**: Each step can be run independently\n",
    "- **Configurable**: Easy parameter adjustment for different experiments\n",
    "- **Reproducible**: Fixed random seeds for consistent results\n",
    "- **Scalable**: Debug mode for quick testing, full mode for production\n",
    "- **Comprehensive**: Saves all intermediate results and trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd7a1b6",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Modules\n",
    "\n",
    "### Library Overview:\n",
    "This section imports all necessary dependencies for the credit risk modeling pipeline.\n",
    "\n",
    "**Core Data Science Libraries:**\n",
    "- `numpy` & `pandas`: Data manipulation and numerical operations\n",
    "- `sklearn`: Machine learning utilities (VarianceThreshold for feature selection)\n",
    "- `pickle` & `json`: Model serialization and configuration storage\n",
    "\n",
    "**Custom Project Modules:**\n",
    "- `src.utils.utils`: Utility functions (random seed, timer, progress tracking)\n",
    "- `src.data_pipeline.processor`: Main data processing orchestrator\n",
    "- `src.processing.*`: Data validation, encoding, and imputation modules\n",
    "- `src.modeling.*`: Feature selection and stacking model implementations\n",
    "\n",
    "### Prerequisites:\n",
    "Ensure all custom modules are properly installed and the project structure is maintained. The pipeline depends on these modules being in the correct relative paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5ca314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Thêm đường dẫn gốc của project vào sys.path để import được module src\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab80a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Import custom modules\n",
    "from src.utils.utils import set_seed, timer, create_progress_bar\n",
    "from src.data_pipeline.processor import DataProcessor\n",
    "from src.processing.data_validator import validate_data\n",
    "from src.processing.encoding import TargetEncoder\n",
    "from src.processing.imputation import SimpleImputer\n",
    "from src.modeling.feature_selector import FeatureSelector\n",
    "from src.modeling.stacking import run_l1_stacking, run_l2_stacking, run_l3_stacking, print_all_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20051b53",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup\n",
    "\n",
    "### Pipeline Configuration Parameters:\n",
    "\n",
    "**Execution Modes:**\n",
    "- `DEBUG = False`: \n",
    "  - **False**: Full dataset processing (production mode)\n",
    "  - **True**: Limited to 10,000 samples for quick testing and debugging\n",
    "\n",
    "**Reproducibility:**\n",
    "- `SEED = 42`: Fixed random seed ensures consistent results across runs\n",
    "\n",
    "**Data Handling:**\n",
    "- `FORCE_RELOAD = True`: Forces fresh data loading, ignoring cached files\n",
    "- `SKIP_SHAP = False`: \n",
    "  - **False**: Use SHAP for intelligent feature selection (slower, better quality)\n",
    "  - **True**: Use variance-based selection (faster, good for debugging)\n",
    "\n",
    "**Model Training:**\n",
    "- `USE_ENSEMBLE = False`: Reserved for future ensemble extensions\n",
    "- `TUNE_HYPERPARAMS = True`: \n",
    "  - **True**: Perform hyperparameter optimization (recommended for final models)\n",
    "  - **False**: Use default parameters (faster for testing)\n",
    "\n",
    "### Recommendations:\n",
    "- For **development/testing**: Set DEBUG=True, SKIP_SHAP=True, TUNE_HYPERPARAMS=False\n",
    "- For **production runs**: Set DEBUG=False, SKIP_SHAP=False, TUNE_HYPERPARAMS=True\n",
    "- For **quick experiments**: Set DEBUG=True, TUNE_HYPERPARAMS=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaddc2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline configuration\n",
    "DEBUG = False  # Set to True for quick testing with subset of data\n",
    "SEED = 42\n",
    "FORCE_RELOAD = True\n",
    "SKIP_SHAP = False  # Set to True to use variance-based feature selection instead of SHAP\n",
    "USE_ENSEMBLE = False\n",
    "TUNE_HYPERPARAMS = True\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "print(f\"Pipeline Configuration:\")\n",
    "print(f\"- Seed: {SEED}\")\n",
    "print(f\"- Debug mode: {DEBUG}\")\n",
    "print(f\"- Force reload: {FORCE_RELOAD}\")\n",
    "print(f\"- Skip SHAP: {SKIP_SHAP}\")\n",
    "print(f\"- Use Ensemble: {USE_ENSEMBLE}\")\n",
    "print(f\"- Tune Hyperparams: {TUNE_HYPERPARAMS}\")\n",
    "\n",
    "if DEBUG:\n",
    "    print(\"\\nDEMO MODE: Running with first 10,000 rows only!\")\n",
    "else:\n",
    "    print(\"\\nFULL MODE: Running with complete dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad360de9",
   "metadata": {},
   "source": [
    "## 2.1. Temporary Output Configuration\n",
    "\n",
    "### Safe Testing Environment:\n",
    "To run the pipeline without affecting existing results, we'll use a temporary output directory.\n",
    "\n",
    "**Benefits:**\n",
    "- **Safe Testing**: Original results remain untouched\n",
    "- **Clean Separation**: Clear distinction between test and production runs\n",
    "- **Easy Cleanup**: Simple to remove test results when done\n",
    "- **Comparison Ready**: Can compare new results with existing ones\n",
    "\n",
    "**Configuration:**\n",
    "- All output files will be saved to `temp_results/` directory\n",
    "- Original file structure is preserved within the temporary directory\n",
    "- Easy to copy results back to main directories if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76788f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using temporary output directories under 'notebooks/temp_results/'\n",
      "✓ Original results will NOT be affected\n",
      "Output directories configured:\n",
      "- Interim data: notebooks/temp_results/data/interim\n",
      "- Processed data: notebooks/temp_results/data/processed\n",
      "- L1 models: notebooks/temp_results/models/l1_stacking\n",
      "- L2 models: notebooks/temp_results/models/l2_stacking\n",
      "- L3 models: notebooks/temp_results/models/l3_stacking\n"
     ]
    }
   ],
   "source": [
    "# Configure temporary output directories\n",
    "USE_TEMP_OUTPUT = True  # Set to False to use original directories\n",
    "\n",
    "if USE_TEMP_OUTPUT:\n",
    "    # Create temporary directory structure\n",
    "    TEMP_BASE = 'notebooks/temp_results'\n",
    "    DATA_INTERIM_DIR = f'{TEMP_BASE}/data/interim'\n",
    "    DATA_PROCESSED_DIR = f'{TEMP_BASE}/data/processed'\n",
    "    MODELS_L1_DIR = f'{TEMP_BASE}/models/l1_stacking'\n",
    "    MODELS_L2_DIR = f'{TEMP_BASE}/models/l2_stacking'\n",
    "    MODELS_L3_DIR = f'{TEMP_BASE}/models/l3_stacking'\n",
    "    \n",
    "    # Create all temporary directories\n",
    "    temp_dirs = [DATA_INTERIM_DIR, DATA_PROCESSED_DIR, MODELS_L1_DIR, MODELS_L2_DIR, MODELS_L3_DIR]\n",
    "    for temp_dir in temp_dirs:\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"✓ Using temporary output directories under '{TEMP_BASE}/'\")\n",
    "    print(f\"✓ Original results will NOT be affected\")\n",
    "    \n",
    "else:\n",
    "    # Use original directories\n",
    "    DATA_INTERIM_DIR = 'data/interim'\n",
    "    DATA_PROCESSED_DIR = 'data/processed'\n",
    "    MODELS_L1_DIR = 'models/l1_stacking'\n",
    "    MODELS_L2_DIR = 'models/l2_stacking'\n",
    "    MODELS_L3_DIR = 'models/l3_stacking'\n",
    "    \n",
    "    print(f\"⚠️  Using ORIGINAL output directories - existing results may be overwritten!\")\n",
    "\n",
    "print(f\"Output directories configured:\")\n",
    "print(f\"- Interim data: {DATA_INTERIM_DIR}\")\n",
    "print(f\"- Processed data: {DATA_PROCESSED_DIR}\")\n",
    "print(f\"- L1 models: {MODELS_L1_DIR}\")\n",
    "print(f\"- L2 models: {MODELS_L2_DIR}\")\n",
    "print(f\"- L3 models: {MODELS_L3_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad946fff",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Initial Processing\n",
    "\n",
    "### Data Loading Strategy:\n",
    "The `DataProcessor` class handles the complex task of loading and merging multiple data sources:\n",
    "\n",
    "**Primary Datasets:**\n",
    "- `application_train.csv`: Training data with known TARGET values\n",
    "- `application_test.csv`: Test data requiring predictions\n",
    "\n",
    "**External Data Sources (automatically merged):**\n",
    "- `bureau.csv` & `bureau_balance.csv`: Credit bureau information\n",
    "- `previous_application.csv`: Historical loan applications  \n",
    "- `POS_CASH_balance.csv`: Point-of-sale and cash loan balances\n",
    "- `installments_payments.csv`: Payment history data\n",
    "- `credit_card_balance.csv`: Credit card usage patterns\n",
    "\n",
    "### Data Quality Checks:\n",
    "The loading process includes several validation steps:\n",
    "- Automatic schema validation\n",
    "- Missing value assessment\n",
    "- Data type consistency checks\n",
    "- Merge integrity verification\n",
    "\n",
    "### Expected Outcomes:\n",
    "- Successfully merged dataset combining all data sources\n",
    "- Clear separation of training (with TARGET) and test (without TARGET) rows\n",
    "- Comprehensive feature set ready for engineering phase\n",
    "\n",
    "**Note**: The merge process significantly increases the feature count as it aggregates information from multiple sources for each applicant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89933670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING CREDIT RISK MODELING PIPELINE\n",
      "============================================================\n",
      "Loading data...\n",
      "Before merge: (356255, 122)\n",
      "Số dòng test trước merge: 48744\n",
      "Số dòng test sau merge bureau: 48744\n",
      "Before merge: (356255, 122)\n",
      "Số dòng test trước merge: 48744\n",
      "Số dòng test sau merge bureau: 48744\n",
      "Số dòng test sau merge prev: 48744\n",
      "Lưu cache merge vào data/interim/cache_merged.feather...\n",
      "Số dòng test sau merge: 48744\n",
      "Số dòng test sau merge prev: 48744\n",
      "Lưu cache merge vào data/interim/cache_merged.feather...\n",
      "Số dòng test sau merge: 48744\n",
      "Dataset shape: (356255, 134)\n",
      "Number of test rows after merge: 48744\n",
      "\n",
      "Dataset Info:\n",
      "- Total rows: 356,255\n",
      "- Total columns: 134\n",
      "- Training rows: 307,511\n",
      "- Test rows: 48,744\n",
      "Data Loading - done in 99s\n",
      "Dataset shape: (356255, 134)\n",
      "Number of test rows after merge: 48744\n",
      "\n",
      "Dataset Info:\n",
      "- Total rows: 356,255\n",
      "- Total columns: 134\n",
      "- Training rows: 307,511\n",
      "- Test rows: 48,744\n",
      "Data Loading - done in 99s\n"
     ]
    }
   ],
   "source": [
    "# Change to project root if we're in notebooks folder\n",
    "if os.getcwd().endswith('notebooks'):\n",
    "    os.chdir('..')\n",
    "    print(\"Changed to project root:\", os.getcwd())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING CREDIT RISK MODELING PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with timer(\"Data Loading\"):\n",
    "    print('Loading data...')\n",
    "    processor = DataProcessor(debug=DEBUG, seed=SEED, force_reload=FORCE_RELOAD)\n",
    "    df = processor.load_data()\n",
    "    print(f'Dataset shape: {df.shape}')\n",
    "    print('Number of test rows after merge:', df['TARGET'].isnull().sum())\n",
    "    \n",
    "    # Display basic dataset information\n",
    "    print(f\"\\nDataset Info:\")\n",
    "    print(f\"- Total rows: {len(df):,}\")\n",
    "    print(f\"- Total columns: {len(df.columns):,}\")\n",
    "    print(f\"- Training rows: {df['TARGET'].notnull().sum():,}\")\n",
    "    print(f\"- Test rows: {df['TARGET'].isnull().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bfbb5d",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "### Automated Feature Engineering Process:\n",
    "This step creates new predictive features through sophisticated transformations of the raw data.\n",
    "\n",
    "**Feature Creation Techniques:**\n",
    "\n",
    "1. **Aggregation Features:**\n",
    "   - Statistical summaries (mean, median, std, min, max) of external data\n",
    "   - Count-based features (number of previous loans, credit inquiries)\n",
    "   - Trend analysis (recent vs. historical behavior patterns)\n",
    "\n",
    "2. **Ratio and Interaction Features:**\n",
    "   - Income-to-credit ratios\n",
    "   - Debt-to-income calculations  \n",
    "   - Payment behavior ratios\n",
    "   - Cross-variable interactions\n",
    "\n",
    "3. **Time-Based Features:**\n",
    "   - Days since last application\n",
    "   - Loan duration patterns\n",
    "   - Seasonal payment behaviors\n",
    "   - Application timing features\n",
    "\n",
    "4. **Risk Indicators:**\n",
    "   - Default probability proxies\n",
    "   - Credit utilization patterns\n",
    "   - Payment delay frequencies\n",
    "   - Financial stress indicators\n",
    "\n",
    "### Business Logic Integration:\n",
    "The feature engineering incorporates domain knowledge about credit risk:\n",
    "- **Income Stability**: Regular vs. irregular income patterns\n",
    "- **Credit History**: Length and quality of credit relationships\n",
    "- **Payment Behavior**: Consistency and timeliness of payments\n",
    "- **Financial Burden**: Debt levels relative to income capacity\n",
    "\n",
    "### Expected Impact:\n",
    "Feature engineering typically increases the dataset width significantly (often 2-3x the original feature count) while creating more predictive variables that capture complex relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16d70beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting auto feature engineering...\n",
      "Bắt đầu feature engineering...\n",
      "Lưu cache feature engineering vào data/interim/cache_fe.feather...\n",
      "Lưu cache feature engineering vào data/interim/cache_fe.feather...\n",
      "Dataset shape after feature engineering: (356255, 161)\n",
      "Number of test rows after feature engineering: 48744\n",
      "\n",
      "Feature Engineering Results:\n",
      "- New dataset shape: (356255, 161)\n",
      "Dataset shape after feature engineering: (356255, 161)\n",
      "Number of test rows after feature engineering: 48744\n",
      "\n",
      "Feature Engineering Results:\n",
      "- New dataset shape: (356255, 161)\n",
      "Before merge: (356255, 122)\n",
      "Số dòng test trước merge: 48744\n",
      "Số dòng test sau merge bureau: 48744\n",
      "Before merge: (356255, 122)\n",
      "Số dòng test trước merge: 48744\n",
      "Số dòng test sau merge bureau: 48744\n",
      "Số dòng test sau merge prev: 48744\n",
      "Lưu cache merge vào data/interim/cache_merged.feather...\n",
      "Số dòng test sau merge: 48744\n",
      "Số dòng test sau merge prev: 48744\n",
      "Lưu cache merge vào data/interim/cache_merged.feather...\n",
      "Số dòng test sau merge: 48744\n",
      "- Features added: 27\n",
      "Feature Engineering - done in 80s\n",
      "- Features added: 27\n",
      "Feature Engineering - done in 80s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Feature Engineering\"):\n",
    "    print('Starting auto feature engineering...')\n",
    "    df = processor.auto_feature_engineering(df)\n",
    "    print(f'Dataset shape after feature engineering: {df.shape}')\n",
    "    print('Number of test rows after feature engineering:', df['TARGET'].isnull().sum())\n",
    "    \n",
    "    print(f\"\\nFeature Engineering Results:\")\n",
    "    print(f\"- New dataset shape: {df.shape}\")\n",
    "    print(f\"- Features added: {df.shape[1] - processor.load_data().shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ed617",
   "metadata": {},
   "source": [
    "## 5. Missing Value Handling\n",
    "\n",
    "### Missing Value Strategy:\n",
    "Credit data often contains missing values due to various reasons. Our approach handles them systematically:\n",
    "\n",
    "**Missing Value Categories:**\n",
    "\n",
    "1. **Informative Missingness:**\n",
    "   - Missing values that indicate \"not applicable\" (e.g., no previous loans)\n",
    "   - These are often replaced with meaningful defaults (0, -1, or \"None\")\n",
    "\n",
    "2. **Random Missingness:**\n",
    "   - Missing values due to data collection issues\n",
    "   - Handled through imputation strategies\n",
    "\n",
    "3. **Systematic Missingness:**\n",
    "   - Missing values following patterns (e.g., certain data not collected for specific loan types)\n",
    "   - Require domain-specific handling\n",
    "\n",
    "**Handling Techniques:**\n",
    "- **Numerical Features**: Median imputation, forward/backward fill for time series\n",
    "- **Categorical Features**: Mode imputation or \"Unknown\" category creation  \n",
    "- **Boolean Features**: Conservative assumptions (e.g., False for unknown flags)\n",
    "- **Special Cases**: Domain knowledge-based replacements\n",
    "\n",
    "### Business Considerations:\n",
    "- **Conservative Approach**: When uncertain, assume higher risk scenario\n",
    "- **Feature Preservation**: Maintain feature distribution patterns\n",
    "- **Regulatory Compliance**: Ensure missing value handling doesn't introduce bias\n",
    "\n",
    "**Post-Processing**: Track missing value patterns as they may be predictive features themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b4320d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values...\n",
      "Number of test rows after handling missing values: 48744\n",
      "\n",
      "Missing values summary:\n",
      "- Columns with missing values: 16\n",
      "- Total missing values: 5392569\n",
      "Missing Value Handling - done in 1s\n",
      "Number of test rows after handling missing values: 48744\n",
      "\n",
      "Missing values summary:\n",
      "- Columns with missing values: 16\n",
      "- Total missing values: 5392569\n",
      "Missing Value Handling - done in 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values...\n",
      "Number of test rows after handling missing values: 48744\n",
      "\n",
      "Missing values summary:\n",
      "- Columns with missing values: 16\n",
      "- Total missing values: 5392569\n",
      "Missing Value Handling - done in 1s\n",
      "Number of test rows after handling missing values: 48744\n",
      "\n",
      "Missing values summary:\n",
      "- Columns with missing values: 16\n",
      "- Total missing values: 5392569\n",
      "Missing Value Handling - done in 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\HLC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Missing Value Handling\"):\n",
    "    print('Handling missing values...')\n",
    "    df = processor.handle_missing_values(df)\n",
    "    print('Number of test rows after handling missing values:', df['TARGET'].isnull().sum())\n",
    "    \n",
    "    # Check missing values\n",
    "    missing_counts = df.isnull().sum()\n",
    "    print(f\"\\nMissing values summary:\")\n",
    "    print(f\"- Columns with missing values: {(missing_counts > 0).sum()}\")\n",
    "    print(f\"- Total missing values: {missing_counts.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a978caa",
   "metadata": {},
   "source": [
    "## 6. Data Encoding\n",
    "\n",
    "### Target Encoding for Categorical Variables:\n",
    "Target encoding is particularly effective for credit risk modeling as it captures the relationship between categorical variables and default probability.\n",
    "\n",
    "**Target Encoding Process:**\n",
    "\n",
    "1. **Category-Target Relationship:**\n",
    "   - For each category, calculate the mean TARGET value\n",
    "   - This creates a numerical representation based on historical default rates\n",
    "   - Example: If \"Engineer\" profession has 5% default rate, it gets encoded as 0.05\n",
    "\n",
    "2. **Smoothing and Regularization:**\n",
    "   - Apply smoothing to handle categories with few samples\n",
    "   - Use cross-validation to prevent overfitting\n",
    "   - Add noise to reduce variance in rare categories\n",
    "\n",
    "3. **Advantages over One-Hot Encoding:**\n",
    "   - **Dimensionality**: No explosion of features for high-cardinality categories\n",
    "   - **Information Retention**: Preserves ordinal relationships with target\n",
    "   - **Performance**: Often leads to better model performance\n",
    "\n",
    "**Categories Typically Encoded:**\n",
    "- `NAME_EDUCATION_TYPE`: Education levels and their risk profiles\n",
    "- `OCCUPATION_TYPE`: Job types and associated stability/risk\n",
    "- `NAME_FAMILY_STATUS`: Marital status impact on credit behavior  \n",
    "- `NAME_HOUSING_TYPE`: Housing situation and financial stability\n",
    "- `ORGANIZATION_TYPE`: Employer type and associated risk levels\n",
    "\n",
    "### Train/Test Split Strategy:\n",
    "- Encoding parameters are learned only from training data\n",
    "- Test data is encoded using training-derived mappings\n",
    "- This prevents data leakage and ensures realistic evaluation\n",
    "\n",
    "**Quality Assurance**: Verify that train/test distributions remain consistent after encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5ac89f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting encoding...\n",
      "Train set shape: (307511, 161)\n",
      "Test set shape: (48744, 161)\n",
      "\n",
      "Encoding Results:\n",
      "- Train encoded shape: (307511, 161)\n",
      "- Test encoded shape: (48744, 161)\n",
      "Data Encoding - done in 0s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Data Encoding\"):\n",
    "    print('Starting encoding...')\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    target = 'TARGET'\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    train_df = df[df[target].notnull()]\n",
    "    test_df = df[df[target].isnull()]\n",
    "    print(f'Train set shape: {train_df.shape}')\n",
    "    print(f'Test set shape: {test_df.shape}')\n",
    "    \n",
    "    # Target encoding for categorical variables\n",
    "    encoder = TargetEncoder()\n",
    "    train_encoded = encoder.fit_transform(train_df, target, categorical_cols)\n",
    "    test_encoded = encoder.transform(test_df, categorical_cols)\n",
    "    \n",
    "    print(f\"\\nEncoding Results:\")\n",
    "    print(f\"- Train encoded shape: {train_encoded.shape}\")\n",
    "    print(f\"- Test encoded shape: {test_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d73cf",
   "metadata": {},
   "source": [
    "## 7. Save Interim Data\n",
    "\n",
    "### Data Persistence Strategy:\n",
    "Saving intermediate results serves multiple purposes in the machine learning pipeline:\n",
    "\n",
    "**Benefits of Interim Data Storage:**\n",
    "\n",
    "1. **Pipeline Resilience:**\n",
    "   - Resume processing from this checkpoint if later steps fail\n",
    "   - Skip expensive preprocessing when experimenting with models\n",
    "   - Quick data access for analysis and debugging\n",
    "\n",
    "2. **Reproducibility:**\n",
    "   - Exact same preprocessing can be replicated\n",
    "   - Enables consistent model comparisons\n",
    "   - Facilitates collaboration and code reviews\n",
    "\n",
    "3. **Data Lineage:**\n",
    "   - Track transformation history\n",
    "   - Audit trail for regulatory compliance\n",
    "   - Quality assurance checkpoints\n",
    "\n",
    "**Files Created:**\n",
    "- `data/interim/train_encoded.csv`: Training data with target encoding applied\n",
    "- `data/interim/test_encoded.csv`: Test data with same encoding transformations\n",
    "\n",
    "### Data Quality at This Stage:\n",
    "- All categorical variables converted to numerical representations\n",
    "- Target encoding preserves predictive relationships\n",
    "- Train/test split maintained with proper encoding methodology\n",
    "- Ready for feature selection and model training phases\n",
    "\n",
    "**Next Steps**: These encoded datasets will undergo feature selection to identify the most predictive variables for the final models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d83aa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interim data saved:\n",
      "- notebooks/temp_results/data/interim/train_encoded.csv\n",
      "- notebooks/temp_results/data/interim/test_encoded.csv\n"
     ]
    }
   ],
   "source": [
    "# Save interim data\n",
    "os.makedirs(DATA_INTERIM_DIR, exist_ok=True)\n",
    "train_encoded.to_csv(f'{DATA_INTERIM_DIR}/train_encoded.csv', index=False)\n",
    "test_encoded.to_csv(f'{DATA_INTERIM_DIR}/test_encoded.csv', index=False)\n",
    "\n",
    "print(\"Interim data saved:\")\n",
    "print(f\"- {DATA_INTERIM_DIR}/train_encoded.csv\")\n",
    "print(f\"- {DATA_INTERIM_DIR}/test_encoded.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de43d19b",
   "metadata": {},
   "source": [
    "## 8. Feature Selection\n",
    "\n",
    "### Intelligent Feature Selection Strategy:\n",
    "With hundreds or thousands of features after engineering, selecting the most predictive ones is crucial for model performance and interpretability.\n",
    "\n",
    "**Two-Tier Selection Approach:**\n",
    "\n",
    "### Option A: SHAP-Based Selection (Default - Higher Quality)\n",
    "**SHAP (SHapley Additive exPlanations) Advantages:**\n",
    "- **Model-Agnostic**: Works with any machine learning algorithm\n",
    "- **Theoretically Grounded**: Based on cooperative game theory\n",
    "- **Feature Interactions**: Captures complex feature relationships\n",
    "- **Interpretability**: Provides explanation for feature importance\n",
    "\n",
    "**SHAP Process:**\n",
    "1. **Pre-filtering**: Variance threshold to remove low-variance features\n",
    "2. **SHAP Calculation**: Train lightweight model and compute SHAP values\n",
    "3. **Importance Ranking**: Rank features by absolute SHAP value contribution\n",
    "4. **Correlation Filtering**: Remove highly correlated features (>0.95)\n",
    "5. **Final Selection**: Top 50 features for optimal model complexity\n",
    "\n",
    "### Option B: Variance-Based Selection (Fast Alternative)\n",
    "**When to Use**: Debug mode, quick experiments, or computational constraints\n",
    "- **Speed**: Much faster than SHAP computation\n",
    "- **Simplicity**: Easy to understand and implement\n",
    "- **Baseline**: Good starting point for feature selection\n",
    "\n",
    "### Selection Parameters:\n",
    "- **Target Features**: 50 final features (optimal balance of performance vs. complexity)\n",
    "- **Sample Size**: 5,000 samples for SHAP calculation (computational efficiency)\n",
    "- **Correlation Threshold**: 0.95 (remove redundant features)\n",
    "\n",
    "### Expected Outcomes:\n",
    "- Reduced feature set with highest predictive power\n",
    "- Improved model training speed and performance  \n",
    "- Enhanced model interpretability\n",
    "- Reduced overfitting risk through dimensionality reduction\n",
    "\n",
    "**Business Value**: Selected features represent the most important factors in credit risk assessment, providing clear insights for decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d954c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (307511, 159)\n",
      "Using SHAP-based feature selection...\n",
      "[SHAP] Reducing features from 159 to 100 for SHAP...\n",
      "[SHAP] After variance selection: 100 features\n",
      "[SHAP] Starting model training for feature selection...\n",
      "[SHAP] Training XGBoost model for SHAP...\n",
      "[SHAP] Using sample size 5000...\n",
      "[SHAP] Starting SHAP values calculation on 5000 rows...\n",
      "[SHAP] SHAP values calculation completed.\n",
      "[SHAP] Calculating feature importance...\n",
      "[SHAP] Selecting top 50 features...\n",
      "[SHAP] Top 50 features selected:\n",
      "   1. [ORIG] EXT_SOURCE_2: 0.3401\n",
      "   2. [ORIG] EXT_SOURCE_3: 0.2952\n",
      "   3. [ORIG] EXT_SOURCE_1: 0.1349\n",
      "   4. [ORIG] CREDIT_TO_ANNUITY: 0.1262\n",
      "   5. [ORIG] CODE_GENDER: 0.1142\n",
      "   6. [ORIG] DAYS_EMPLOYED: 0.0905\n",
      "   7. [ORIG] CREDIT_GOODS_RATIO: 0.0900\n",
      "   8. [ORIG] NAME_EDUCATION_TYPE: 0.0885\n",
      "   9. [ORIG] PREV_NAME_CONTRACT_STATUS__lambda_: 0.0714\n",
      "  10. [ORIG] DAYS_BIRTH: 0.0669\n",
      "  11. [ORIG] AMT_GOODS_PRICE: 0.0632\n",
      "  12. [ORIG] FLAG_OWN_CAR: 0.0585\n",
      "  13. [ORIG] DAYS_ID_PUBLISH: 0.0476\n",
      "  14. [ORIG] GOODS_MINUS_CREDIT: 0.0473\n",
      "  15. [ORIG] BURO_CREDIT_ACTIVE__lambda_: 0.0436\n",
      "  16. [ORIG] AMT_ANNUITY: 0.0428\n",
      "  17. [ORIG] AMT_ANNUITY_DIV_EXTSRC3: 0.0426\n",
      "  18. [ORIG] NAME_FAMILY_STATUS: 0.0399\n",
      "  19. [OTHER] OWN_CAR_AGE: 0.0371\n",
      "  20. [ORIG] BURO_DAYS_CREDIT_mean: 0.0352\n",
      "  21. [ORIG] NAME_INCOME_TYPE: 0.0350\n",
      "  22. [ORIG] FLAG_DOCUMENT_3: 0.0340\n",
      "  23. [ORIG] FLAG_WORK_PHONE: 0.0331\n",
      "  24. [ORIG] BURO_DAYS_CREDIT_max: 0.0327\n",
      "  25. [ORIG] PREV_AMT_CREDIT_sum: 0.0315\n",
      "  26. [ORIG] DEF_30_CNT_SOCIAL_CIRCLE: 0.0273\n",
      "  27. [OTHER] REGION_RATING_CLIENT_W_CITY: 0.0269\n",
      "  28. [ORIG] DAYS_LAST_PHONE_CHANGE: 0.0268\n",
      "  29. [ORIG] ANNUITY_INCOME_RATIO: 0.0263\n",
      "  30. [ORIG] AMT_REQ_CREDIT_BUREAU_QRT: 0.0259\n",
      "  31. [ORIG] BURO_AMT_CREDIT_SUM_max: 0.0247\n",
      "  32. [ORIG] PREV_AMT_CREDIT_max: 0.0238\n",
      "  33. [ORIG] ORGANIZATION_TYPE: 0.0210\n",
      "  34. [ORIG] PREV_AMT_CREDIT_mean: 0.0190\n",
      "  35. [ORIG] BURO_AMT_CREDIT_SUM_sum: 0.0187\n",
      "  36. [OTHER] OCCUPATION_TYPE: 0.0178\n",
      "  37. [ORIG] AMT_CREDIT: 0.0174\n",
      "  38. [OTHER] AGE_YEARS: 0.0156\n",
      "  39. [ORIG] BURO_AMT_CREDIT_SUM_mean: 0.0153\n",
      "  40. [ORIG] CREDIT_INCOME_RATIO: 0.0132\n",
      "  41. [ORIG] NAME_CONTRACT_TYPE: 0.0129\n",
      "  42. [OTHER] REG_CITY_NOT_LIVE_CITY: 0.0125\n",
      "  43. [ORIG] DAYS_REGISTRATION: 0.0117\n",
      "  44. [OTHER] WALLSMATERIAL_MODE: 0.0109\n",
      "  45. [OTHER] WEEKDAY_APPR_PROCESS_START: 0.0093\n",
      "  46. [ORIG] BURO_DAYS_CREDIT_min: 0.0091\n",
      "  47. [OTHER] LIVINGAREA_MEDI: 0.0088\n",
      "  48. [ORIG] DEF_60_CNT_SOCIAL_CIRCLE: 0.0077\n",
      "  49. [OTHER] REGION_CAT: 0.0069\n",
      "  50. [OTHER] ENTRANCES_MODE: 0.0067\n",
      "[SHAP] Summary: 40 original, 10 other\n",
      "[SHAP] Filtering features based on correlation...\n",
      "[SHAP] Removing 2 features with high correlation: ['AMT_CREDIT', 'AGE_YEARS']...\n",
      "[SHAP] Finally selected 48 features\n",
      "Feature Selection - done in 7s\n",
      "\n",
      "Selected Features (48):\n",
      "  1. EXT_SOURCE_2\n",
      "  2. EXT_SOURCE_3\n",
      "  3. EXT_SOURCE_1\n",
      "  4. CREDIT_TO_ANNUITY\n",
      "  5. CODE_GENDER\n",
      "  6. DAYS_EMPLOYED\n",
      "  7. CREDIT_GOODS_RATIO\n",
      "  8. NAME_EDUCATION_TYPE\n",
      "  9. PREV_NAME_CONTRACT_STATUS__lambda_\n",
      "  10. DAYS_BIRTH\n",
      "  ... and 38 more features\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Feature Selection\"):\n",
    "    X_train = train_encoded.drop(columns=[target, 'SK_ID_CURR'])\n",
    "    y_train = train_encoded[target]\n",
    "    print(f'X_train shape: {X_train.shape}')\n",
    "    \n",
    "    if SKIP_SHAP:\n",
    "        print('Using variance-based feature selection...')\n",
    "        # Use variance-based selection for faster processing\n",
    "        selector_var = VarianceThreshold()\n",
    "        selector_var.fit(X_train.fillna(0))\n",
    "        variances = selector_var.variances_\n",
    "        top_idx = np.argsort(variances)[-50:]  # Select top 50 features\n",
    "        selected_features = X_train.columns[top_idx]\n",
    "        print(f'Selected {len(selected_features)} features based on variance')\n",
    "        \n",
    "    else:\n",
    "        print('Using SHAP-based feature selection...')\n",
    "        \n",
    "        # Reduce number of features for SHAP computation\n",
    "        if X_train.shape[1] > 100:\n",
    "            print(f'[SHAP] Reducing features from {X_train.shape[1]} to 100 for SHAP...')\n",
    "            selector_var = VarianceThreshold()\n",
    "            selector_var.fit(X_train.fillna(0))\n",
    "            variances = selector_var.variances_\n",
    "            top_idx = np.argsort(variances)[-100:]\n",
    "            X_train = X_train.iloc[:, top_idx]\n",
    "            test_encoded = test_encoded[X_train.columns]\n",
    "            print(f'[SHAP] After variance selection: {X_train.shape[1]} features')\n",
    "        \n",
    "        # SHAP feature selection\n",
    "        selector = FeatureSelector(\n",
    "            top_k=50,\n",
    "            seed=SEED, \n",
    "            shap_sample=5000,\n",
    "            full_shap=False\n",
    "        )\n",
    "        selected_features = selector.fit(X_train, y_train)\n",
    "        \n",
    "        # Additional feature filtering based on correlation\n",
    "        print('[SHAP] Filtering features based on correlation...')\n",
    "        X_selected = X_train[selected_features]\n",
    "        if not isinstance(X_selected, pd.DataFrame):\n",
    "            X_selected = pd.DataFrame(X_selected, columns=selected_features)\n",
    "        corr_matrix = X_selected.corr().abs()\n",
    "        \n",
    "        # Remove features with high correlation (>0.95)\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "        \n",
    "        if to_drop:\n",
    "            print(f'[SHAP] Removing {len(to_drop)} features with high correlation: {to_drop[:5]}...')\n",
    "            selected_features = [f for f in selected_features if f not in to_drop]\n",
    "        \n",
    "        # Limit back to 50 final features\n",
    "        if len(selected_features) > 50:\n",
    "            selected_features = selected_features[:50]\n",
    "        \n",
    "        print(f'[SHAP] Finally selected {len(selected_features)} features')\n",
    "\n",
    "print(f\"\\nSelected Features ({len(selected_features)}):\")\n",
    "for i, feature in enumerate(selected_features[:10]):\n",
    "    print(f\"  {i+1}. {feature}\")\n",
    "if len(selected_features) > 10:\n",
    "    print(f\"  ... and {len(selected_features) - 10} more features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d18574",
   "metadata": {},
   "source": [
    "## 9. Data Imputation\n",
    "\n",
    "### Final Data Preparation - Advanced Imputation:\n",
    "After feature selection, we apply sophisticated imputation techniques to handle any remaining missing values in our selected feature set.\n",
    "\n",
    "**Why Imputation After Feature Selection:**\n",
    "- **Efficiency**: Only impute the features that will actually be used\n",
    "- **Quality**: Focus computational resources on important features\n",
    "- **Consistency**: Ensure imputation strategy aligns with selected features\n",
    "\n",
    "**SimpleImputer Strategy:**\n",
    "The `SimpleImputer` class employs intelligent imputation based on feature types:\n",
    "\n",
    "1. **Numerical Features:**\n",
    "   - **Median Imputation**: Robust to outliers\n",
    "   - **Distribution Preservation**: Maintains feature distribution characteristics\n",
    "   - **Cross-Validation**: Ensures imputation doesn't introduce bias\n",
    "\n",
    "2. **Categorical Features:**\n",
    "   - **Mode Imputation**: Most frequent category\n",
    "   - **Special Categories**: \"Unknown\" for truly missing information\n",
    "   - **Frequency-Based**: Consider category frequency in imputation\n",
    "\n",
    "3. **Business Rules:**\n",
    "   - **Conservative Estimates**: When uncertain, assume higher risk scenario\n",
    "   - **Domain Knowledge**: Use credit industry best practices\n",
    "   - **Regulatory Compliance**: Ensure fairness and non-discrimination\n",
    "\n",
    "### Data Quality Assurance:\n",
    "- **Pre-Imputation**: Record missing value patterns\n",
    "- **Post-Imputation**: Validate data distributions\n",
    "- **Consistency Check**: Ensure train/test imputation alignment\n",
    "\n",
    "### Final Data Preparation:\n",
    "- **Format Consistency**: Ensure DataFrame format for downstream processing\n",
    "- **Column Ordering**: Maintain consistent feature order\n",
    "- **Type Validation**: Verify all features are properly formatted\n",
    "\n",
    "**Result**: Clean, complete dataset ready for machine learning model training with no missing values and optimal feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c5a3fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting imputation...\n",
      "Imputation completed\n",
      "Final processed data shapes:\n",
      "- X_train_selected: (307511, 48)\n",
      "- X_test_selected: (48744, 48)\n",
      "Data Imputation - done in 0s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Data Imputation\"):\n",
    "    X_train_selected = X_train[selected_features]\n",
    "    X_test_selected = test_encoded[selected_features]\n",
    "    \n",
    "    print('Starting imputation...')\n",
    "    imputer = SimpleImputer()\n",
    "    imputer.fit(X_train_selected)\n",
    "    X_train_selected = imputer.transform(X_train_selected)\n",
    "    X_test_selected = imputer.transform(X_test_selected)\n",
    "    print('Imputation completed')\n",
    "    \n",
    "    # Ensure DataFrames\n",
    "    if not isinstance(X_train_selected, pd.DataFrame):\n",
    "        if isinstance(selected_features, pd.Index):\n",
    "            selected_features = selected_features.tolist()\n",
    "        X_train_selected = pd.DataFrame(X_train_selected, columns=selected_features)\n",
    "    if not isinstance(X_test_selected, pd.DataFrame):\n",
    "        if isinstance(selected_features, pd.Index):\n",
    "            selected_features = selected_features.tolist()\n",
    "        X_test_selected = pd.DataFrame(X_test_selected, columns=selected_features)\n",
    "    \n",
    "    print(f\"Final processed data shapes:\")\n",
    "    print(f\"- X_train_selected: {X_train_selected.shape}\")\n",
    "    print(f\"- X_test_selected: {X_test_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d76aa40",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data\n",
    "\n",
    "### Final Data Artifacts - Production Ready:\n",
    "This checkpoint saves the fully processed, model-ready datasets that represent the culmination of all preprocessing steps.\n",
    "\n",
    "**Critical Data Assets Created:**\n",
    "\n",
    "### `data/processed/train_processed.csv`:\n",
    "- **Complete Training Set**: All preprocessing applied + TARGET column\n",
    "- **Model Ready**: Can be directly used for model training\n",
    "- **Feature Complete**: 50 selected features + target variable\n",
    "- **Quality Assured**: No missing values, optimal data types\n",
    "\n",
    "### `data/processed/test_processed.csv`:\n",
    "- **Prediction Ready**: Test set with identical preprocessing\n",
    "- **Consistent Transformation**: Same encoding, selection, and imputation as training\n",
    "- **Production Format**: Ready for final model predictions\n",
    "\n",
    "### Data Validation Process:\n",
    "The `validate_data()` function performs comprehensive quality checks:\n",
    "\n",
    "**Training Data Validation:**\n",
    "- **Target Distribution**: Verify class balance and target variable integrity\n",
    "- **Feature Completeness**: Ensure no missing values remain\n",
    "- **Data Types**: Confirm all features are numerical and model-compatible\n",
    "- **Statistical Sanity**: Check for reasonable value ranges\n",
    "\n",
    "**Test Data Validation:**\n",
    "- **Schema Consistency**: Same features as training data\n",
    "- **Distribution Alignment**: Similar feature distributions to training\n",
    "- **Format Compatibility**: Ready for model inference\n",
    "\n",
    "### Business Impact:\n",
    "- **Reproducibility**: Exact same data can be used across experiments\n",
    "- **Auditability**: Complete preprocessing audit trail\n",
    "- **Efficiency**: Skip preprocessing in future model iterations\n",
    "- **Collaboration**: Standardized data format for team use\n",
    "\n",
    "**Next Phase**: These processed datasets feed directly into the three-level stacking model training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "979cdb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved:\n",
      "- notebooks/temp_results/data/processed/train_processed.csv\n",
      "- notebooks/temp_results/data/processed/test_processed.csv\n",
      "\n",
      "=== Training Data Validation ===\n",
      "NaN values: 0\n",
      "Infinity values: 0\n",
      "Min value: -540000.000000\n",
      "Max value: 1017957888.000000\n",
      "Zero variance features: 0\n",
      "Target distribution: {0.0: 282686, 1.0: 24825}\n",
      "==============================\n",
      "\n",
      "=== Test Data Validation ===\n",
      "NaN values: 0\n",
      "Infinity values: 0\n",
      "Min value: -356400.000000\n",
      "Max value: 609164480.000000\n",
      "Zero variance features: 0\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Save fully processed data\n",
    "os.makedirs(DATA_PROCESSED_DIR, exist_ok=True)\n",
    "X_train_processed = X_train_selected.copy()\n",
    "X_train_processed['TARGET'] = y_train.values\n",
    "X_train_processed.to_csv(f'{DATA_PROCESSED_DIR}/train_processed.csv', index=False)\n",
    "X_test_selected.to_csv(f'{DATA_PROCESSED_DIR}/test_processed.csv', index=False)\n",
    "\n",
    "print(\"Processed data saved:\")\n",
    "print(f\"- {DATA_PROCESSED_DIR}/train_processed.csv\")\n",
    "print(f\"- {DATA_PROCESSED_DIR}/test_processed.csv\")\n",
    "\n",
    "# Data quality validation\n",
    "validate_data(X_train_selected, y_train, \"Training Data\")\n",
    "validate_data(X_test_selected, None, \"Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19834846",
   "metadata": {},
   "source": [
    "## 11. Level 1 Stacking (Base Models)\n",
    "\n",
    "### Foundation Layer - Diverse Base Model Ensemble:\n",
    "Level 1 represents the foundation of our stacking approach, employing three complementary algorithms that capture different aspects of the data.\n",
    "\n",
    "**Base Model Architecture:**\n",
    "\n",
    "### XGBoost (Extreme Gradient Boosting):\n",
    "- **Strengths**: Excellent handling of missing values, built-in regularization\n",
    "- **Best For**: Non-linear relationships, feature interactions\n",
    "- **Credit Risk Advantage**: Robust to outliers, handles mixed data types well\n",
    "- **Hyperparameter Focus**: Learning rate, max depth, regularization parameters\n",
    "\n",
    "### LightGBM (Light Gradient Boosting Machine):\n",
    "- **Strengths**: Fast training, memory efficient, high accuracy\n",
    "- **Best For**: Large datasets, categorical features\n",
    "- **Credit Risk Advantage**: Efficient handling of high-cardinality categorical variables\n",
    "- **Hyperparameter Focus**: Number of leaves, feature fraction, bagging parameters\n",
    "\n",
    "### CatBoost (Categorical Boosting):\n",
    "- **Strengths**: Superior categorical feature handling, minimal hyperparameter tuning\n",
    "- **Best For**: Datasets with many categorical features, robust performance\n",
    "- **Credit Risk Advantage**: Built-in categorical encoding, reduces preprocessing needs\n",
    "- **Hyperparameter Focus**: Iterations, depth, learning rate\n",
    "\n",
    "### Cross-Validation Strategy:\n",
    "- **5-Fold Stratified CV**: Maintains class distribution across folds\n",
    "- **Out-of-Fold Predictions**: Generate unbiased predictions for meta-learning\n",
    "- **Performance Tracking**: Individual model AUC scores for model selection\n",
    "\n",
    "### Output Artifacts:\n",
    "- **Trained Models**: Serialized models for each algorithm (.pkl files)\n",
    "- **OOF Predictions**: Out-of-fold predictions for Level 2 training\n",
    "- **Test Predictions**: Base predictions for final ensemble\n",
    "- **Performance Metrics**: AUC, accuracy, and other evaluation metrics\n",
    "\n",
    "### Ensemble Benefit:\n",
    "Combining diverse algorithms reduces overfitting and captures different data patterns:\n",
    "- **XGBoost**: Captures complex interactions\n",
    "- **LightGBM**: Efficient pattern recognition  \n",
    "- **CatBoost**: Robust categorical handling\n",
    "\n",
    "**Expected Performance**: Each base model should achieve competitive individual performance, with the ensemble providing superior results through diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe6a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"L1 Stacking\"):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"LEVEL 1 STACKING - BASE MODELS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    models_l1, oof_preds_l1, test_preds_l1, metrics_l1 = run_l1_stacking(\n",
    "        X_train_selected, y_train, X_test_selected, TUNE_HYPERPARAMS\n",
    "    )\n",
    "    \n",
    "    # Save L1 models and predictions\n",
    "    l1_dir = MODELS_L1_DIR\n",
    "    os.makedirs(l1_dir, exist_ok=True)\n",
    "    expected_l1_models = ['xgb', 'lgbm', 'catboost']\n",
    "    \n",
    "    for name in expected_l1_models:\n",
    "        # Save model\n",
    "        if name in models_l1 and models_l1[name] is not None:\n",
    "            with open(f'{l1_dir}/l1_{name}_model.pkl', 'wb') as f:\n",
    "                pickle.dump(models_l1[name], f)\n",
    "        \n",
    "        # Save OOF predictions\n",
    "        if name in oof_preds_l1:\n",
    "            pd.DataFrame({'oof_preds': oof_preds_l1[name]}).to_csv(\n",
    "                f'{l1_dir}/l1_{name}_oof_predictions.csv', index=False\n",
    "            )\n",
    "        \n",
    "        # Save test predictions\n",
    "        if name in test_preds_l1:\n",
    "            pd.DataFrame({'test_preds': test_preds_l1[name]}).to_csv(\n",
    "                f'{l1_dir}/l1_{name}_test_predictions.csv', index=False\n",
    "            )\n",
    "    \n",
    "    # Save metrics summary\n",
    "    with open(f'{l1_dir}/l1_model_summary.json', 'w') as f:\n",
    "        json.dump(metrics_l1, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nL1 models saved to {l1_dir}/\")\n",
    "    print(\"L1 Model Performance:\")\n",
    "    for name, metric in metrics_l1.items():\n",
    "        if isinstance(metric, dict) and 'auc' in metric:\n",
    "            print(f\"  - {name}: AUC = {metric['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ebe5ed",
   "metadata": {},
   "source": [
    "## 12. Level 2 Stacking (Meta Models)\n",
    "\n",
    "### Intelligence Layer - Meta-Learning from Base Predictions:\n",
    "Level 2 models learn how to optimally combine the Level 1 predictions, acting as intelligent arbitrators that understand when each base model performs best.\n",
    "\n",
    "**Meta-Learning Architecture:**\n",
    "\n",
    "### ExtraTrees (Extremely Randomized Trees):\n",
    "- **Meta-Learning Strength**: Captures non-linear combinations of base predictions\n",
    "- **Ensemble Intelligence**: Learns complex interaction patterns between L1 models\n",
    "- **Overfitting Resistance**: High randomness reduces overfitting to L1 patterns\n",
    "- **Feature Handling**: Can incorporate both L1 predictions and original features\n",
    "\n",
    "**Why ExtraTrees for Meta-Learning:**\n",
    "- **Stability**: Less sensitive to small changes in base predictions\n",
    "- **Interpretability**: Can analyze which base models contribute most\n",
    "- **Performance**: Often excels at combining diverse predictions\n",
    "\n",
    "### Logistic Regression:\n",
    "- **Meta-Learning Strength**: Linear combination of base predictions with clear weights\n",
    "- **Interpretability**: Coefficients show relative importance of each base model\n",
    "- **Calibration**: Excellent probability calibration for credit risk scores\n",
    "- **Simplicity**: Stable, interpretable baseline meta-learner\n",
    "\n",
    "**Why Logistic Regression for Meta-Learning:**\n",
    "- **Probability Focus**: Natural for credit risk probability estimation\n",
    "- **Regulatory Friendly**: Highly interpretable for compliance requirements\n",
    "- **Calibration**: Well-calibrated probabilities crucial for credit decisions\n",
    "\n",
    "### Meta-Learning Strategy:\n",
    "**Input Features for L2 Models:**\n",
    "1. **L1 Predictions**: Out-of-fold predictions from XGBoost, LightGBM, CatBoost\n",
    "2. **Original Features**: Selected raw features for additional context\n",
    "3. **Prediction Confidence**: Variance measures from L1 models (optional)\n",
    "\n",
    "**Training Process:**\n",
    "- **Clean Training Data**: Use OOF predictions to avoid overfitting\n",
    "- **Feature Engineering**: Create interaction terms between L1 predictions\n",
    "- **Cross-Validation**: Generate L2 OOF predictions for L3 training\n",
    "\n",
    "### Expected Meta-Learning Benefits:\n",
    "- **Model Selection**: Learn when each L1 model is most reliable\n",
    "- **Prediction Refinement**: Correct systematic errors from L1 models\n",
    "- **Uncertainty Quantification**: Better calibrated probability estimates\n",
    "- **Performance Gain**: Typically 1-3% AUC improvement over best single model\n",
    "\n",
    "### Ensemble Intelligence:\n",
    "The L2 layer creates an intelligent voting system that:\n",
    "- **Adapts to Data Regions**: Different weights for different input patterns\n",
    "- **Handles Model Bias**: Corrects for individual model weaknesses  \n",
    "- **Improves Calibration**: Better probability estimates for business use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9921eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"L2 Stacking\"):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"LEVEL 2 STACKING - META MODELS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    models_l2, oof_preds_l2, test_preds_l2, metrics_l2 = run_l2_stacking(\n",
    "        y_train, X_train_selected, X_test_selected\n",
    "    )\n",
    "    \n",
    "    # Save L2 models and predictions\n",
    "    l2_dir = MODELS_L2_DIR\n",
    "    os.makedirs(l2_dir, exist_ok=True)\n",
    "    expected_l2_models = ['extratree', 'logistic']\n",
    "    \n",
    "    for name in expected_l2_models:\n",
    "        # Save model\n",
    "        if name in models_l2 and models_l2[name] is not None:\n",
    "            with open(f'{l2_dir}/l2_{name}_model.pkl', 'wb') as f:\n",
    "                pickle.dump(models_l2[name], f)\n",
    "        \n",
    "        # Save OOF predictions\n",
    "        if name in oof_preds_l2:\n",
    "            pd.DataFrame({'oof_preds': oof_preds_l2[name]}).to_csv(\n",
    "                f'{l2_dir}/l2_{name}_oof_predictions.csv', index=False\n",
    "            )\n",
    "        \n",
    "        # Save test predictions\n",
    "        if name in test_preds_l2:\n",
    "            pd.DataFrame({'test_preds': test_preds_l2[name]}).to_csv(\n",
    "                f'{l2_dir}/l2_{name}_test_predictions.csv', index=False\n",
    "            )\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(f'{l2_dir}/l2_model_summary.json', 'w') as f:\n",
    "        json.dump(metrics_l2, f, indent=2)\n",
    "    \n",
    "    # Check predictions\n",
    "    for name in expected_l2_models:\n",
    "        if name not in oof_preds_l2:\n",
    "            print(f\"WARNING: L2 model '{name}' did NOT produce OOF predictions!\")\n",
    "        else:\n",
    "            print(f\"OK: L2 model '{name}' OOF predictions found, length = {len(oof_preds_l2[name])}\")\n",
    "    \n",
    "    # Blend test predictions\n",
    "    blended_test_pred_l2 = np.mean([v for v in test_preds_l2.values()], axis=0)\n",
    "    pd.DataFrame({'blended_test_pred': blended_test_pred_l2}).to_csv(\n",
    "        f'{l2_dir}/l2_blended_test_predictions.csv', index=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nL2 models saved to {l2_dir}/\")\n",
    "    print(\"L2 Model Performance:\")\n",
    "    for name, metric in metrics_l2.items():\n",
    "        if isinstance(metric, dict) and 'auc' in metric:\n",
    "            print(f\"  - {name}: AUC = {metric['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c292f59",
   "metadata": {},
   "source": [
    "## 13. Level 3 Stacking (Final Ensemble)\n",
    "\n",
    "### Supreme Decision Layer - Ultimate Model Synthesis:\n",
    "Level 3 represents the pinnacle of our stacking approach, combining the intelligent meta-predictions from L2 with carefully selected raw features to create the final, most sophisticated model.\n",
    "\n",
    "**Final Ensemble Architecture:**\n",
    "\n",
    "### ExtraTrees as Final Arbiter:\n",
    "**Why ExtraTrees for L3:**\n",
    "- **Complex Pattern Recognition**: Captures intricate relationships between L2 predictions\n",
    "- **Feature Integration**: Seamlessly combines meta-predictions with raw features\n",
    "- **Overfitting Resistance**: Random feature selection reduces overfitting risk\n",
    "- **Non-Linear Mastery**: Learns complex decision boundaries for final classification\n",
    "\n",
    "### Multi-Source Input Strategy:\n",
    "**L3 Model Input Features:**\n",
    "\n",
    "1. **L2 Meta-Predictions:**\n",
    "   - ExtraTrees probability scores (L2 model 1)\n",
    "   - Logistic Regression probability scores (L2 model 2)\n",
    "   - These capture the refined intelligence from base model combinations\n",
    "\n",
    "2. **Strategic Raw Features:**\n",
    "   - `AMT_INCOME_TOTAL`: Fundamental creditworthiness indicator\n",
    "   - Additional key features that provide direct business insight\n",
    "   - Features that complement meta-predictions with raw signal\n",
    "\n",
    "3. **Feature Interaction Potential:**\n",
    "   - L3 can learn interactions between meta-predictions and raw features\n",
    "   - Example: Income level might moderate the reliability of certain model predictions\n",
    "\n",
    "### Advanced Learning Capabilities:\n",
    "**L3 Intelligence Beyond L2:**\n",
    "- **Meta-Meta Learning**: Learns when L2 models are most reliable\n",
    "- **Context Awareness**: Adjusts predictions based on raw feature context\n",
    "- **Boundary Refinement**: Fine-tunes decision boundaries using all available information\n",
    "- **Confidence Calibration**: Final probability calibration for business use\n",
    "\n",
    "### Expected Performance Benefits:\n",
    "- **Incremental Improvement**: Additional 0.5-1% AUC gain over L2\n",
    "- **Enhanced Stability**: More robust predictions across different data segments\n",
    "- **Business Alignment**: Incorporates both model intelligence and domain features\n",
    "- **Regulatory Compliance**: Maintains interpretability through feature transparency\n",
    "\n",
    "### Final Model Characteristics:\n",
    "- **Ensemble Depth**: Three levels of model sophistication\n",
    "- **Feature Diversity**: Meta-predictions + raw business features\n",
    "- **Risk Calibration**: Well-calibrated probabilities for credit decisions\n",
    "- **Production Ready**: Single model file for deployment\n",
    "\n",
    "### Business Value:\n",
    "The L3 final ensemble represents the culmination of advanced machine learning techniques while maintaining business interpretability and regulatory compliance requirements for credit risk assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9da78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"L3 Stacking\"):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"LEVEL 3 STACKING - FINAL ENSEMBLE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    l3_dir = MODELS_L3_DIR\n",
    "    os.makedirs(l3_dir, exist_ok=True)\n",
    "    l2_model_names = ['extratree', 'logistic']\n",
    "    raw_feature_names = []\n",
    "    \n",
    "    if 'AMT_INCOME_TOTAL' in X_train_selected.columns:\n",
    "        raw_feature_names.append('AMT_INCOME_TOTAL')\n",
    "    \n",
    "    # Run L3 stacking\n",
    "    model_l3, oof_preds_l3, test_preds_l3, metrics_l3 = run_l3_stacking(\n",
    "        y_train,\n",
    "        test_df,\n",
    "        l2_model_names,\n",
    "        X_train_selected,\n",
    "        X_test_selected,\n",
    "        raw_feature_names\n",
    "    )\n",
    "    \n",
    "    # Save L3 model and predictions\n",
    "    with open(f'{l3_dir}/l3_extratree_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model_l3, f)\n",
    "    \n",
    "    pd.DataFrame({'oof_preds': oof_preds_l3}).to_csv(\n",
    "        f'{l3_dir}/l3_extratree_oof_predictions.csv', index=False\n",
    "    )\n",
    "    pd.DataFrame({'test_preds': test_preds_l3}).to_csv(\n",
    "        f'{l3_dir}/l3_extratree_test_predictions.csv', index=False\n",
    "    )\n",
    "    \n",
    "    with open(f'{l3_dir}/l3_model_summary.json', 'w') as f:\n",
    "        json.dump(metrics_l3, f, indent=2)\n",
    "    \n",
    "    # Check L3 predictions\n",
    "    if oof_preds_l3 is None or len(oof_preds_l3) == 0:\n",
    "        print(\"WARNING: L3 model did NOT produce OOF predictions!\")\n",
    "    else:\n",
    "        print(f\"OK: L3 model OOF predictions found, length = {len(oof_preds_l3)}\")\n",
    "    \n",
    "    # Save final submission\n",
    "    submission_df = pd.DataFrame({\n",
    "        'SK_ID_CURR': test_df['SK_ID_CURR'].reset_index(drop=True), \n",
    "        'TARGET': test_preds_l3\n",
    "    })\n",
    "    submission_df.to_csv(f'{l3_dir}/submission_l3.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nL3 model saved to {l3_dir}/\")\n",
    "    print(f\"Final submission saved to {l3_dir}/submission_l3.csv\")\n",
    "    \n",
    "    if isinstance(metrics_l3, dict) and 'auc' in metrics_l3:\n",
    "        print(f\"L3 Final Model AUC: {metrics_l3['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f5e44",
   "metadata": {},
   "source": [
    "## 14. Final Performance Summary\n",
    "\n",
    "### Comprehensive Model Evaluation & Business Impact:\n",
    "This section provides a complete performance overview across all model levels and delivers the final business outcomes.\n",
    "\n",
    "**Performance Analysis Hierarchy:**\n",
    "\n",
    "### Model Performance Comparison:\n",
    "**Individual Model Assessment:**\n",
    "- **L1 Base Models**: XGBoost, LightGBM, CatBoost individual AUC scores\n",
    "- **L2 Meta Models**: ExtraTrees, Logistic Regression meta-learning performance  \n",
    "- **L3 Final Ensemble**: Ultimate model performance with all optimizations\n",
    "\n",
    "**Key Performance Metrics:**\n",
    "- **AUC-ROC**: Primary metric for ranking and discrimination ability\n",
    "- **Precision/Recall**: Business-relevant performance for different thresholds\n",
    "- **Calibration Quality**: How well predicted probabilities match actual default rates\n",
    "- **Stability**: Performance consistency across different data segments\n",
    "\n",
    "### Complete Artifact Inventory:\n",
    "**Data Assets:**\n",
    "- **Interim Data**: Encoded datasets for reproducibility\n",
    "- **Processed Data**: Final model-ready datasets\n",
    "- **Feature Engineering**: Comprehensive feature transformation record\n",
    "\n",
    "**Model Assets:**\n",
    "- **L1 Models**: Three base models with individual predictions\n",
    "- **L2 Models**: Two meta-models with ensemble predictions  \n",
    "- **L3 Model**: Final ensemble model for production deployment\n",
    "\n",
    "**Prediction Assets:**\n",
    "- **Training Predictions**: Out-of-fold predictions for model validation\n",
    "- **Test Predictions**: Final submission-ready predictions\n",
    "- **Model Summaries**: Performance metrics and hyperparameters\n",
    "\n",
    "### Business Delivery:\n",
    "**Primary Deliverable**: `submission_l3.csv`\n",
    "- **Format**: SK_ID_CURR (Customer ID) + TARGET (Default Probability)\n",
    "- **Quality**: Sophisticated ensemble predictions with advanced feature engineering\n",
    "- **Calibration**: Well-calibrated probabilities for business decision-making\n",
    "- **Coverage**: Complete test set predictions ready for submission\n",
    "\n",
    "### Production Readiness:\n",
    "**Model Deployment Assets:**\n",
    "- **Trained Models**: Complete pipeline with all preprocessing and models\n",
    "- **Feature Pipeline**: Reproducible feature engineering and selection\n",
    "- **Prediction Pipeline**: End-to-end inference capability\n",
    "- **Performance Benchmarks**: Established baselines for monitoring\n",
    "\n",
    "### Success Metrics:\n",
    "- **Technical Success**: Pipeline completion without errors\n",
    "- **Performance Success**: Competitive AUC scores across all model levels\n",
    "- **Business Success**: Interpretable, well-calibrated risk predictions\n",
    "- **Operational Success**: Production-ready model artifacts and documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7049c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE EXECUTION COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nFINAL PERFORMANCE SUMMARY:\")\n",
    "print_all_auc(y_train)\n",
    "\n",
    "print(f\"\\nOUTPUT FILES GENERATED:\")\n",
    "print(f\"├── {DATA_INTERIM_DIR}/\")\n",
    "print(f\"│   ├── train_encoded.csv\")\n",
    "print(f\"│   └── test_encoded.csv\")\n",
    "print(f\"├── {DATA_PROCESSED_DIR}/\")\n",
    "print(f\"│   ├── train_processed.csv\")\n",
    "print(f\"│   └── test_processed.csv\")\n",
    "print(f\"├── {MODELS_L1_DIR}/\")\n",
    "print(f\"│   ├── Model files and predictions for XGB, LGBM, CatBoost\")\n",
    "print(f\"├── {MODELS_L2_DIR}/\")\n",
    "print(f\"│   ├── Model files and predictions for ExtraTrees, Logistic\")\n",
    "print(f\"└── {MODELS_L3_DIR}/\")\n",
    "print(f\"    ├── l3_extratree_model.pkl\")\n",
    "print(f\"    └── submission_l3.csv (FINAL SUBMISSION)\")\n",
    "\n",
    "print(f\"\\nSUBMISSION FILE: {MODELS_L3_DIR}/submission_l3.csv\")\n",
    "print(f\"Number of test predictions: {len(test_preds_l3):,}\")\n",
    "print(f\"Prediction range: [{np.min(test_preds_l3):.6f}, {np.max(test_preds_l3):.6f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d4862b",
   "metadata": {},
   "source": [
    "### Pipeline Execution Complete!\n",
    "\n",
    "### Mission Accomplished - Advanced Credit Risk Modeling Pipeline\n",
    "The sophisticated three-level stacking pipeline has been successfully executed, delivering state-of-the-art credit risk predictions through advanced machine learning techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Achievements:**\n",
    "\n",
    "### **Technical Excellence:**\n",
    "- **Complete Data Pipeline**: From raw data to production-ready predictions\n",
    "- **Advanced Feature Engineering**: Sophisticated feature creation and selection  \n",
    "- **Three-Level Stacking**: Hierarchical ensemble for maximum performance\n",
    "- **Model Diversity**: Multiple algorithms capturing different data patterns\n",
    "- **Quality Assurance**: Comprehensive validation and error checking\n",
    "\n",
    "### **Business Value Delivered:**\n",
    "- **High-Quality Predictions**: Well-calibrated default probability estimates\n",
    "- **Model Interpretability**: Clear feature importance and model explanations\n",
    "- **Production Ready**: Complete pipeline ready for deployment\n",
    "- **Performance Optimized**: Advanced ensemble techniques for superior accuracy\n",
    "- **Regulatory Compliant**: Transparent and auditable modeling approach\n",
    "\n",
    "---\n",
    "\n",
    "## **Complete Output Inventory:**\n",
    "\n",
    "### **Data Assets:**\n",
    "- `data/interim/`: Encoded and preprocessed datasets for reproducibility\n",
    "- `data/processed/`: Final model-ready datasets with optimal feature selection\n",
    "\n",
    "### **Model Hierarchy:**\n",
    "- `models/l1_stacking/`: **Base Models** (XGBoost, LightGBM, CatBoost)\n",
    "- `models/l2_stacking/`: **Meta Models** (ExtraTrees, Logistic Regression)  \n",
    "- `models/l3_stacking/`: **Final Ensemble** (Ultimate stacking model)\n",
    "\n",
    "### **Business Deliverables:**\n",
    "- **`submission_l3.csv`**: **PRIMARY DELIVERABLE** - Final credit risk predictions\n",
    "- **Performance Reports**: Comprehensive model evaluation metrics\n",
    "- **Model Artifacts**: Trained models ready for production deployment\n",
    "\n",
    "---\n",
    "\n",
    "## **Next Steps & Recommendations:**\n",
    "\n",
    "### **Immediate Actions:**\n",
    "1. **Review Performance Metrics**: Analyze AUC scores and model comparisons\n",
    "2. **Validate Predictions**: Spot-check prediction quality and calibration\n",
    "3. **Submit Results**: Deploy `submission_l3.csv` for final evaluation\n",
    "4. **Document Insights**: Capture key learnings and feature importance\n",
    "\n",
    "### **Advanced Extensions:**\n",
    "1. **Hyperparameter Optimization**: Fine-tune individual model parameters\n",
    "2. **Feature Engineering++**: Explore additional feature interactions\n",
    "3. **Model Interpretability**: Deep-dive into SHAP explanations\n",
    "4. **Production Pipeline**: Set up automated retraining and monitoring\n",
    "\n",
    "### **Business Applications:**\n",
    "1. **Risk Assessment**: Use predictions for loan approval decisions\n",
    "2. **Portfolio Analysis**: Analyze risk distribution across customer segments  \n",
    "3. **Pricing Strategy**: Incorporate risk scores into loan pricing models\n",
    "4. **Performance Monitoring**: Track model performance over time\n",
    "\n",
    "---\n",
    "\n",
    "## **Excellence Delivered:**\n",
    "This pipeline represents a comprehensive, production-grade credit risk modeling solution that combines:\n",
    "- **Advanced Machine Learning**: State-of-the-art ensemble techniques\n",
    "- **Business Intelligence**: Domain-aware feature engineering and selection\n",
    "- **Operational Excellence**: Complete, reproducible, and scalable pipeline\n",
    "- **Regulatory Compliance**: Transparent and interpretable modeling approach\n",
    "\n",
    "**Final Submission Ready**: The final submission file is saved to the temporary directory and contains sophisticated, well-calibrated credit risk predictions ready for business deployment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
